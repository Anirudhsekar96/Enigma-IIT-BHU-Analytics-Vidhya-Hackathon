{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error,accuracy_score,f1_score\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error,log_loss\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_user_data_train = pd.read_csv(\"train/user_data.csv\")\n",
    "df_train_submission = pd.read_csv(\"train/train_submissions.csv\")\n",
    "df_problem_data_train = pd.read_csv(\"train/problem_data.csv\")\n",
    "df_sample_sub = pd.read_csv(\"sample_submissions_wbscxqU.csv\")\n",
    "df_test_sub = pd.read_csv(\"test_submissions_NeDLEvX.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def mcor(y_true, y_pred):\n",
    "     #matthews_correlation\n",
    "     y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "     y_pred_neg = 1 - y_pred_pos\n",
    " \n",
    " \n",
    "     y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "     y_neg = 1 - y_pos\n",
    " \n",
    " \n",
    "     tp = K.sum(y_pos * y_pred_pos)\n",
    "     tn = K.sum(y_neg * y_pred_neg)\n",
    " \n",
    " \n",
    "     fp = K.sum(y_neg * y_pred_pos)\n",
    "     fn = K.sum(y_pos * y_pred_neg)\n",
    " \n",
    " \n",
    "     numerator = (tp * tn - fp * fn)\n",
    "     denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    " \n",
    " \n",
    "     return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_problem_data_train.drop(['tags'],axis=1,inplace=True)\n",
    "#df_problem_data_train.points.fillna(0,inplace=True)\n",
    "df_problem_data_train.level_type.fillna('N',inplace=True)\n",
    "df_user_data_train.country.fillna('Missing',inplace=True)\n",
    "\n",
    "def postive_only(x):\n",
    "    if(x<0):\n",
    "        return(-1*x)\n",
    "    else:\n",
    "        return(x)\n",
    "\n",
    "def get_points(x1):\n",
    "    if(x1==\"A\"):\n",
    "        return(500)\n",
    "    elif(x1=='B'):\n",
    "        return(1000)\n",
    "    elif(x1=='C'):\n",
    "        return(1500)\n",
    "    elif(x1=='D'):\n",
    "        return(2000)\n",
    "    elif(x1=='E'):\n",
    "        return(2500)\n",
    "    elif(x1=='F'):\n",
    "        return(3000)\n",
    "    elif(x1=='G'):\n",
    "        return(3500)\n",
    "    elif(x1=='H'):\n",
    "        return(4000)\n",
    "    else:\n",
    "        return(1000)\n",
    "\n",
    "df_problem_data_train['points']=df_problem_data_train['level_type'].map(lambda x: get_points(x))\n",
    "df_user_data_train['submission_count']=df_user_data_train['submission_count'].map(lambda x: postive_only(x))\n",
    "df_user_data_train['problem_solved']=df_user_data_train['problem_solved'].map(lambda x: postive_only(x))\n",
    "df_user_data_train['contribution']=df_user_data_train['contribution'].map(lambda x: postive_only(x))\n",
    "df_user_data_train['follower_count']=df_user_data_train['follower_count'].map(lambda x: postive_only(x))\n",
    "df_user_data_train['last_online_time_seconds']=df_user_data_train['last_online_time_seconds'].map(lambda x: postive_only(x))\n",
    "df_user_data_train['max_rating']=df_user_data_train['max_rating'].map(lambda x: postive_only(x))\n",
    "df_user_data_train['registration_time_seconds']=df_user_data_train['registration_time_seconds'].map(lambda x: postive_only(x))\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#lb = LabelEncoder()\n",
    "#df_user_data_train['country'] = lb.fit_transform(df_user_data_train['country'])\n",
    "#df_user_data_train['rank'] = lb.fit_transform(df_user_data_train['rank'])\n",
    "#df_problem_data_train['level_type'] = lb.fit_transform(df_problem_data_train['level_type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_user_data_train['problem_sub_ratio'] = df_user_data_train['problem_solved'] / df_user_data_train['submission_count']\n",
    "\n",
    "df_user_data_train['last_online_time_seconds'] = df_user_data_train.last_online_time_seconds.max() - df_user_data_train['last_online_time_seconds']\n",
    "df_user_data_train['last_online_time_minutes'] = df_user_data_train['last_online_time_seconds'] / 60\n",
    "df_user_data_train['last_online_time_hours'] = df_user_data_train['last_online_time_seconds'] / (60*60)\n",
    "df_user_data_train['last_online_time_days'] = df_user_data_train['last_online_time_seconds'] / (60*60*24)\n",
    "df_user_data_train['last_online_time_months'] = df_user_data_train['last_online_time_seconds'] / (60*60*24*30)\n",
    "df_user_data_train['last_online_time_years'] = df_user_data_train['last_online_time_seconds'] / (60*60*24*365)\n",
    "\n",
    "df_user_data_train['registration_time_seconds'] = df_user_data_train.registration_time_seconds.max() - df_user_data_train['registration_time_seconds']\n",
    "df_user_data_train['registration_time_minutes'] = df_user_data_train['registration_time_seconds'] / 60\n",
    "df_user_data_train['registration_time_hours'] = df_user_data_train['registration_time_seconds'] / (60*60)\n",
    "df_user_data_train['registration_time_days'] = df_user_data_train['registration_time_seconds'] / (60*60*24)\n",
    "df_user_data_train['registration_time_months'] = df_user_data_train['registration_time_seconds'] / (60*60*24*30)\n",
    "df_user_data_train['registration_time_years'] = df_user_data_train['registration_time_seconds'] / (60*60*24*365)\n",
    "\n",
    "df_user_data_train['last_vs_reg_time_ratio'] = df_user_data_train['last_online_time_seconds']/(1+df_user_data_train['registration_time_seconds'])\n",
    "\n",
    "df_user_data_train['max_rating_current_ratio'] = df_user_data_train['max_rating']/ (1+df_user_data_train['rating'])\n",
    "df_user_data_train['current_rating_per_registration_time'] = df_user_data_train['rating']/(1+df_user_data_train['registration_time_days'])\n",
    "df_user_data_train['Follower_vs_Rating'] = df_user_data_train['follower_count']/(1+df_user_data_train['rating'])\n",
    "df_user_data_train['Rating_mul_Follower'] = df_user_data_train['rating'] * df_user_data_train['follower_count']\n",
    "df_user_data_train['follower_vs_Contribution'] = df_user_data_train['follower_count']/(1+df_user_data_train['contribution'])\n",
    "\n",
    "\n",
    "\n",
    "df_problem_data_train = pd.get_dummies(df_problem_data_train,columns=['level_type'])\n",
    "df_user_data_train = pd.get_dummies(df_user_data_train,columns=['country','rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train  = pd.merge(df_train_submission,df_user_data_train, how='left', on='user_id')\n",
    "df_train = pd.merge(df_train,df_problem_data_train,how='left',on='problem_id')\n",
    "\n",
    "df_test  = pd.merge(df_test_sub,df_user_data_train, how='left', on='user_id')\n",
    "df_test = pd.merge(df_test,df_problem_data_train,how='left',on='problem_id')\n",
    "\n",
    "X = df_train.drop(['attempts_range','user_id','problem_id'],axis=1).values\n",
    "y = df_train.attempts_range.values\n",
    "\n",
    "X_test = df_test.drop(['ID','user_id','problem_id'],axis=1).values\n",
    "\n",
    "ss = StandardScaler()\n",
    "X = ss.fit_transform(X)\n",
    "X_test_sub = ss.transform(X_test)\n",
    "\n",
    "dummy_y = np_utils.to_categorical(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1200, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(640, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(dummy_y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 104047 samples, validate on 51248 samples\n",
      "Epoch 1/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.3680 - f1: nan - val_loss: 1.2001 - val_f1: 0.4655\n",
      "Epoch 2/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1810 - f1: 0.4458 - val_loss: 1.1663 - val_f1: 0.4477\n",
      "Epoch 3/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1573 - f1: 0.4545 - val_loss: 1.1560 - val_f1: 0.4585\n",
      "Epoch 4/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1480 - f1: 0.4566 - val_loss: 1.1526 - val_f1: 0.4653\n",
      "Epoch 5/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1441 - f1: 0.4577 - val_loss: 1.1505 - val_f1: 0.4562\n",
      "Epoch 6/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1407 - f1: 0.4581 - val_loss: 1.1505 - val_f1: 0.4428\n",
      "Epoch 7/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1390 - f1: 0.4562 - val_loss: 1.1496 - val_f1: 0.4483\n",
      "Epoch 8/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1375 - f1: 0.4577 - val_loss: 1.1498 - val_f1: 0.4546\n",
      "Epoch 9/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1361 - f1: 0.4573 - val_loss: 1.1496 - val_f1: 0.4660\n",
      "Epoch 10/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1346 - f1: 0.4603 - val_loss: 1.1507 - val_f1: 0.4528\n",
      "Epoch 11/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1337 - f1: 0.4579 - val_loss: 1.1526 - val_f1: 0.4319\n",
      "Epoch 12/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1333 - f1: 0.4600 - val_loss: 1.1514 - val_f1: 0.4554\n",
      "Epoch 13/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1321 - f1: 0.4588 - val_loss: 1.1522 - val_f1: 0.4606\n",
      "Epoch 14/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1312 - f1: 0.4622 - val_loss: 1.1501 - val_f1: 0.4526\n",
      "Epoch 15/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1289 - f1: 0.4586 - val_loss: 1.1509 - val_f1: 0.4472\n",
      "Epoch 16/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1278 - f1: 0.4619 - val_loss: 1.1511 - val_f1: 0.4636\n",
      "Epoch 17/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1271 - f1: 0.4620 - val_loss: 1.1523 - val_f1: 0.4465\n",
      "Epoch 18/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1259 - f1: 0.4626 - val_loss: 1.1527 - val_f1: 0.4420\n",
      "Epoch 19/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1249 - f1: 0.4611 - val_loss: 1.1550 - val_f1: 0.4329\n",
      "Epoch 20/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1247 - f1: 0.4602 - val_loss: 1.1581 - val_f1: 0.3952\n",
      "Epoch 21/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1248 - f1: 0.4541 - val_loss: 1.1542 - val_f1: 0.4621\n",
      "Epoch 22/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1218 - f1: 0.4626 - val_loss: 1.1539 - val_f1: 0.4570\n",
      "Epoch 23/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1201 - f1: 0.4656 - val_loss: 1.1555 - val_f1: 0.4596\n",
      "Epoch 24/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1196 - f1: 0.4660 - val_loss: 1.1554 - val_f1: 0.4400\n",
      "Epoch 25/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1178 - f1: 0.4660 - val_loss: 1.1573 - val_f1: 0.4469\n",
      "Epoch 26/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1165 - f1: 0.4621 - val_loss: 1.1590 - val_f1: 0.4647\n",
      "Epoch 27/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1159 - f1: 0.4692 - val_loss: 1.1584 - val_f1: 0.4428\n",
      "Epoch 28/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1150 - f1: 0.4682 - val_loss: 1.1627 - val_f1: 0.4421\n",
      "Epoch 29/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1139 - f1: 0.4699 - val_loss: 1.1588 - val_f1: 0.4339\n",
      "Epoch 30/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1119 - f1: 0.4677 - val_loss: 1.1640 - val_f1: 0.4687\n",
      "Epoch 31/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1125 - f1: 0.4706 - val_loss: 1.1612 - val_f1: 0.4506\n",
      "Epoch 32/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1095 - f1: 0.4716 - val_loss: 1.1616 - val_f1: 0.4496\n",
      "Epoch 33/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1073 - f1: 0.4693 - val_loss: 1.1703 - val_f1: 0.4873\n",
      "Epoch 34/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1078 - f1: 0.4694 - val_loss: 1.1638 - val_f1: 0.4522\n",
      "Epoch 35/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1048 - f1: 0.4715 - val_loss: 1.1764 - val_f1: 0.3996\n",
      "Epoch 36/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1055 - f1: 0.4673 - val_loss: 1.1664 - val_f1: 0.4495\n",
      "Epoch 37/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1002 - f1: 0.4763 - val_loss: 1.1710 - val_f1: 0.4124\n",
      "Epoch 38/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1006 - f1: 0.4706 - val_loss: 1.1686 - val_f1: 0.4366\n",
      "Epoch 39/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.1005 - f1: 0.4715 - val_loss: 1.1723 - val_f1: 0.4171\n",
      "Epoch 40/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.0965 - f1: 0.4744 - val_loss: 1.1710 - val_f1: 0.4565\n",
      "Epoch 41/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.0946 - f1: 0.4791 - val_loss: 1.1745 - val_f1: 0.4367\n",
      "Epoch 42/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.0931 - f1: 0.4759 - val_loss: 1.1756 - val_f1: 0.4534\n",
      "Epoch 43/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.0918 - f1: 0.4801 - val_loss: 1.1782 - val_f1: 0.4276\n",
      "Epoch 44/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.0908 - f1: 0.4763 - val_loss: 1.1789 - val_f1: 0.4266\n",
      "Epoch 45/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.0900 - f1: 0.4782 - val_loss: 1.1822 - val_f1: 0.4661\n",
      "Epoch 46/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.0871 - f1: 0.4781 - val_loss: 1.1796 - val_f1: 0.4458\n",
      "Epoch 47/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.0864 - f1: 0.4805 - val_loss: 1.1814 - val_f1: 0.4453\n",
      "Epoch 48/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.0842 - f1: 0.4816 - val_loss: 1.1890 - val_f1: 0.4708\n",
      "Epoch 49/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.0839 - f1: 0.4815 - val_loss: 1.1876 - val_f1: 0.4605\n",
      "Epoch 50/50\n",
      "104047/104047 [==============================] - 0s - loss: 1.0871 - f1: 0.4792 - val_loss: 1.1884 - val_f1: 0.4498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f98a80c3650>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10000,validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_sub,batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = y_pred.argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_sub['ID'] = df_test.ID\n",
    "df_sample_sub['attempts_range'] = y_pred_val\n",
    "df_sample_sub.to_csv(\"Keras_with_Basic__FE.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
